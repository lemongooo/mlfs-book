{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f44e17-ed2d-47e7-887d-c07490a50f83",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Colab Users - Uncomment & Run the following 2 Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499d2ef2-19de-4245-835e-5ba9c7c351c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.20\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751b756f-b210-429e-bec6-1e67659fe349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: hopsworks 4.1.0\n",
      "Uninstalling hopsworks-4.1.0:\n",
      "  Successfully uninstalled hopsworks-4.1.0\n",
      "\u001b[33mWARNING: Skipping hsfs as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping hsml as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall hopsworks hsfs hsml -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af29988a-0c89-4ae1-a390-1235daaf64c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hopsworks\n",
      "  Using cached hopsworks-4.1.4-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyhumps==1.6.1 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (1.6.1)\n",
      "Requirement already satisfied: requests in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (2.32.3)\n",
      "Requirement already satisfied: furl in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (2.1.3)\n",
      "Requirement already satisfied: boto3 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (1.35.70)\n",
      "Requirement already satisfied: pandas<2.2.0 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (2.1.4)\n",
      "Requirement already satisfied: pyjks in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (20.0.0)\n",
      "Requirement already satisfied: mock in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (5.1.0)\n",
      "Requirement already satisfied: avro==1.11.3 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (1.11.3)\n",
      "Requirement already satisfied: sqlalchemy in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (2.0.29)\n",
      "Requirement already satisfied: PyMySQL[rsa] in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (1.1.1)\n",
      "Requirement already satisfied: tzlocal in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (5.2)\n",
      "Requirement already satisfied: fsspec in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (2024.10.0)\n",
      "Requirement already satisfied: retrying in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (1.3.4)\n",
      "Requirement already satisfied: hopsworks_aiomysql==0.2.1 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks_aiomysql[sa]==0.2.1->hopsworks) (0.2.1)\n",
      "Requirement already satisfied: opensearch-py<=2.4.2,>=1.1.0 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (2.4.2)\n",
      "Requirement already satisfied: tqdm in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (4.67.1)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.49.1 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (1.68.0)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.25.4 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from hopsworks) (4.25.5)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (1.26.20)\n",
      "Requirement already satisfied: six in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2024.8.30)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from pandas<2.2.0->hopsworks) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from pandas<2.2.0->hopsworks) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from pandas<2.2.0->hopsworks) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->hopsworks) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->hopsworks) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from sqlalchemy->hopsworks) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from sqlalchemy->hopsworks) (3.1.1)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.70 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from boto3->hopsworks) (1.35.70)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from boto3->hopsworks) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from boto3->hopsworks) (0.10.4)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from furl->hopsworks) (1.0.1)\n",
      "Requirement already satisfied: javaobj-py3 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from pyjks->hopsworks) (0.4.4)\n",
      "Requirement already satisfied: pyasn1>=0.3.5 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from pyjks->hopsworks) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from pyjks->hopsworks) (0.4.1)\n",
      "Requirement already satisfied: pycryptodomex in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from pyjks->hopsworks) (3.21.0)\n",
      "Requirement already satisfied: twofish in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from pyjks->hopsworks) (0.3.0)\n",
      "Requirement already satisfied: cryptography in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from PyMySQL[rsa]->hopsworks) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from cryptography->PyMySQL[rsa]->hopsworks) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages (from cffi>=1.12->cryptography->PyMySQL[rsa]->hopsworks) (2.21)\n",
      "Using cached hopsworks-4.1.4-py3-none-any.whl (640 kB)\n",
      "Installing collected packages: hopsworks\n",
      "Successfully installed hopsworks-4.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e353e1e-ace8-4ee5-9bef-86a9155e764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hopsworks --quiet\n",
    "!pip install xgboost==2.0.3 --quiet\n",
    "!pip install scikit-learn==1.4.1.post1 --quiet\n",
    "!pip install langchain==0.1.10 --quiet\n",
    "!pip install bitsandbytes==0.42.0 --quiet\n",
    "!pip install accelerate==0.27.2 --quiet\n",
    "!pip install transformers==4.38.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ebc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c75092c-6e9b-4bad-9ebc-b0405f914dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from typing import Any, Dict, List\n",
      "import datetime\n",
      "import pandas as pd\n",
      "import hopsworks\n",
      "from hsfs.feature import Feature\n",
      "\n",
      "def get_historical_data_for_date(date: str, feature_view, weather_fg, model) -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Retrieve data for a specific date from a feature view.\n",
      "\n",
      "    Args:\n",
      "        date (str): The date in the format \"%Y-%m-%d\".\n",
      "        feature_view: The feature view object.\n",
      "        model: The machine learning model used for prediction.\n",
      "\n",
      "    Returns:\n",
      "        pd.DataFrame: A DataFrame containing data for the specified date.\n",
      "    \"\"\"\n",
      "    # Convert date string to datetime object\n",
      "    date_datetime = datetime.datetime.strptime(date, \"%Y-%m-%d\").date()\n",
      "\n",
      "    features_df, labels_df = feature_view.training_data(\n",
      "        start_time=date_datetime,\n",
      "        end_time=date_datetime + datetime.timedelta(days=1),\n",
      "        # event_time=True,\n",
      "        statistics_config=False\n",
      "    )\n",
      "    # bugfix line, shouldn't need to cast to datetime\n",
      "    features_df['date'] = pd.to_datetime(features_df['date'])\n",
      "    batch_data = features_df\n",
      "    batch_data['pm25'] = labels_df['pm25']\n",
      "    batch_data['date'] = batch_data['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
      "\n",
      "    return batch_data[['date', 'pm25']].sort_values('date').reset_index(drop=True)\n",
      "\n",
      "\n",
      "def get_historical_data_in_date_range(date_start: str, date_end: str, feature_view,  weather_fg, model) -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Retrieve data for a specific date range from a time in the past from a feature view.\n",
      "\n",
      "    Args:\n",
      "        date_start (str): The start date in the format \"%Y-%m-%d\".\n",
      "        date_end (str): The end date in the format \"%Y-%m-%d\".\n",
      "        feature_view: The feature view object.\n",
      "        model: The machine learning model used for prediction.\n",
      "\n",
      "    Returns:\n",
      "        pd.DataFrame: A DataFrame containing data for the specified date range.\n",
      "    \"\"\"\n",
      "    # Convert date strings to datetime objects\n",
      "#     date_start_dt = datetime.datetime.strptime(date_start, \"%Y-%m-%d\").date()\n",
      "#     date_end_dt = datetime.datetime.strptime(date_end, \"%Y-%m-%d\").date()\n",
      "\n",
      "    batch_data = feature_view.query.read()\n",
      "    batch_data = batch_data[(batch_data['date'] >= date_start) & (batch_data['date'] <= date_end)]\n",
      "\n",
      "    batch_data['date'] = batch_data['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
      "\n",
      "    return batch_data[['date', 'pm25']].sort_values('date').reset_index(drop=True)\n",
      "\n",
      "def get_future_data_for_date(date: str, feature_view,  weather_fg, model) -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Predicts future PM2.5 data for a specified date using a given feature view and model.\n",
      "\n",
      "    Args:\n",
      "        date (str): The date in the format \"%Y-%m-%d\".\n",
      "        feature_view: The feature view object.\n",
      "        model: The machine learning model used for prediction.\n",
      "\n",
      "    Returns:\n",
      "        pd.DataFrame: A DataFrame containing data for the specified date.\n",
      "    \"\"\"\n",
      "    date_start_dt = datetime.datetime.strptime(date, \"%Y-%m-%d\") #.date()\n",
      "    fg_data = weather_fg.read()\n",
      "\n",
      "    # Couldn't get our filters to work, so filter in memory\n",
      "    df = fg_data[fg_data.date == date_start_dt]\n",
      "    batch_data = df.drop(['date', 'city'], axis=1)\n",
      "\n",
      "    df['pm25'] = model.predict(batch_data)\n",
      "\n",
      "    return df[['date', 'pm25']].sort_values('date').reset_index(drop=True)\n",
      "\n",
      "\n",
      "\n",
      "def get_future_data_in_date_range(date_start: str, date_end: str, feature_view,  weather_fg, model) -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Predicts future PM2.5 data for a specified start and end date range using a given feature view and model.\n",
      "\n",
      "    Args:\n",
      "        date_start (str): The start date in the format \"%Y-%m-%d\".\n",
      "        date_end (str): The end date in the format \"%Y-%m-%d\".\n",
      "        feature_view: The feature view object.\n",
      "        model: The machine learning model used for prediction.\n",
      "\n",
      "    Returns:\n",
      "        pd.DataFrame: A DataFrame containing data for the specified date range.\n",
      "    \"\"\"\n",
      "    date_start_dt = datetime.datetime.strptime(date_start, \"%Y-%m-%d\") #.date()\n",
      "    if date_end == None:\n",
      "        date_end = date_start\n",
      "    date_end_dt = datetime.datetime.strptime(date_end, \"%Y-%m-%d\") #.date()\n",
      "\n",
      "    fg_data = weather_fg.read()\n",
      "    # Fix bug: Cannot compare tz-naive and tz-aware datetime-like objects\n",
      "    fg_data['date'] = pd.to_datetime(fg_data['date']).dt.tz_localize(None)\n",
      "\n",
      "    # Couldn't get our filters to work, so filter in memory\n",
      "    df = fg_data[(fg_data['date'] >= date_start_dt) & (fg_data['date'] <= date_end_dt)]\n",
      "    batch_data = df.drop(['date', 'city'], axis=1)\n",
      "\n",
      "    df['pm25'] = model.predict(batch_data)\n",
      "\n",
      "    return df[['date', 'pm25']].sort_values('date').reset_index(drop=True)\n",
      "import xml.etree.ElementTree as ET\n",
      "import re\n",
      "import inspect\n",
      "from typing import get_type_hints\n",
      "import json\n",
      "import datetime\n",
      "import torch\n",
      "import sys\n",
      "import pandas as pd\n",
      "from openai import OpenAI\n",
      "from functions.air_quality_data_retrieval import (\n",
      "    get_historical_data_for_date,\n",
      "    get_historical_data_in_date_range,\n",
      "    get_future_data_in_date_range,\n",
      "    get_future_data_for_date,\n",
      ")\n",
      "from typing import Any, Dict, List\n",
      "\n",
      "\n",
      "def get_type_name(t: Any) -> str:\n",
      "    \"\"\"Get the name of the type.\"\"\"\n",
      "    name = str(t)\n",
      "    if \"list\" in name or \"dict\" in name:\n",
      "        return name\n",
      "    else:\n",
      "        return t.__name__\n",
      "\n",
      "\n",
      "def serialize_function_to_json(func: Any) -> str:\n",
      "    \"\"\"Serialize a function to JSON.\"\"\"\n",
      "    signature = inspect.signature(func)\n",
      "    type_hints = get_type_hints(func)\n",
      "\n",
      "    function_info = {\n",
      "        \"name\": func.__name__,\n",
      "        \"description\": func.__doc__,\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {}\n",
      "        },\n",
      "        \"returns\": type_hints.get('return', 'void').__name__\n",
      "    }\n",
      "\n",
      "    for name, _ in signature.parameters.items():\n",
      "        param_type = get_type_name(type_hints.get(name, type(None)))\n",
      "        function_info[\"parameters\"][\"properties\"][name] = {\"type\": param_type}\n",
      "\n",
      "    return json.dumps(function_info, indent=2)\n",
      "\n",
      "\n",
      "def get_function_calling_prompt(user_query):\n",
      "    fn = \"\"\"{\"name\": \"function_name\", \"arguments\": {\"arg_1\": \"value_1\", \"arg_2\": value_2, ...}}\"\"\"\n",
      "    example = \"\"\"{\"name\": \"get_historical_data_in_date_range\", \"arguments\": {\"date_start\": \"2024-01-10\", \"date_end\": \"2024-01-14\"}}\"\"\"\n",
      "\n",
      "    prompt = f\"\"\"<|im_start|>system\n",
      "You are a helpful assistant with access to the following functions:\n",
      "\n",
      "{serialize_function_to_json(get_historical_data_for_date)}\n",
      "\n",
      "{serialize_function_to_json(get_historical_data_in_date_range)}\n",
      "\n",
      "{serialize_function_to_json(get_future_data_for_date)}\n",
      "\n",
      "{serialize_function_to_json(get_future_data_in_date_range)}\n",
      "\n",
      "###INSTRUCTIONS:\n",
      "- You need to choose one function to use and retrieve paramenters for this function from the user input.\n",
      "- If the user query contains 'will', and specifies a single day or date, use get_future_data_in_date_range function\n",
      "- If the user query contains 'will', and specifies a range of days or dates, use get_future_data_in_date_range function.\n",
      "- If the user query is for future data, but only includes a single day or date, use the get_future_data_in_date_range function,\n",
      "- If the user query contains 'today' or 'yesterday', use get_historical_data_for_date function.\n",
      "- If the user query contains 'tomorrow', use get_future_data_in_date_range function.\n",
      "- If the user query is for historical data, and specifies a range of days or dates, use use get_historical_data_for_date function.\n",
      "- If the user says a day of the week, assume the date of that day is when that day next arrives.\n",
      "- Do not include feature_view and model parameters.\n",
      "- Provide dates STRICTLY in the YYYY-MM-DD format.\n",
      "- Generate an 'No Function needed' string if the user query does not require function calling.\n",
      "\n",
      "IMPORTANT: Today is {datetime.date.today().strftime(\"%A\")}, {datetime.date.today()}.\n",
      "\n",
      "To use one of there functions respond STRICTLY with:\n",
      "<onefunctioncall>\n",
      "    <functioncall> {fn} </functioncall>\n",
      "</onefunctioncall>\n",
      "\n",
      "###EXAMPLES\n",
      "\n",
      "EXAMPLE 1:\n",
      "- User: Hi!\n",
      "- AI Assiatant: No Function needed.\n",
      "\n",
      "EXAMPLE 2:\n",
      "- User: Is this Air Quality level good or bad?\n",
      "- AI Assiatant: No Function needed.\n",
      "\n",
      "EXAMPLE 3:\n",
      "- User: When and what was the minimum air quality from 2024-01-10 till 2024-01-14?\n",
      "- AI Assistant:\n",
      "<onefunctioncall>\n",
      "    <functioncall> {example} </functioncall>\n",
      "</onefunctioncall>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>user\n",
      "{user_query}\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant\"\"\"\n",
      "\n",
      "    return prompt\n",
      "\n",
      "\n",
      "def generate_hermes(user_query: str, model_llm, tokenizer) -> str:\n",
      "    \"\"\"Retrieves a function name and extracts function parameters based on the user query.\"\"\"\n",
      "\n",
      "    prompt = get_function_calling_prompt(user_query)\n",
      "\n",
      "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(model_llm.device)\n",
      "    input_size = tokens.input_ids.numel()\n",
      "    with torch.inference_mode():\n",
      "        generated_tokens = model_llm.generate(\n",
      "            **tokens,\n",
      "            use_cache=True,\n",
      "            do_sample=True,\n",
      "            temperature=0.2,\n",
      "            top_p=1.0,\n",
      "            top_k=0,\n",
      "            max_new_tokens=512,\n",
      "            eos_token_id=tokenizer.eos_token_id,\n",
      "            pad_token_id=tokenizer.eos_token_id,\n",
      "        )\n",
      "\n",
      "    return tokenizer.decode(\n",
      "        generated_tokens.squeeze()[input_size:],\n",
      "        skip_special_tokens=True,\n",
      "    )\n",
      "\n",
      "\n",
      "def function_calling_with_openai(user_query: str, client) -> str:\n",
      "    \"\"\"\n",
      "    Generates a response using OpenAI's chat API.\n",
      "\n",
      "    Args:\n",
      "        user_query (str): The user's query or prompt.\n",
      "        instructions (str): Instructions or context to provide to the GPT model.\n",
      "\n",
      "    Returns:\n",
      "        str: The generated response from the assistant.\n",
      "    \"\"\"\n",
      "\n",
      "    instructions = get_function_calling_prompt(user_query).split('<|im_start|>user')[0]\n",
      "\n",
      "    completion = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo\",\n",
      "        messages=[\n",
      "            {\"role\": \"system\", \"content\": instructions},\n",
      "            {\"role\": \"user\", \"content\": user_query},\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    # Extract and return the assistant's reply from the response\n",
      "    if completion and completion.choices:\n",
      "        last_choice = completion.choices[0]\n",
      "        if last_choice.message:\n",
      "            return last_choice.message.content.strip()\n",
      "    return \"\"\n",
      "\n",
      "\n",
      "def extract_function_calls(completion: str) -> List[Dict[str, Any]]:\n",
      "    \"\"\"Extract function calls from completion.\"\"\"\n",
      "    completion = completion.strip()\n",
      "    pattern = r\"(<onefunctioncall>(.*?)</onefunctioncall>)\"\n",
      "    match = re.search(pattern, completion, re.DOTALL)\n",
      "    if not match:\n",
      "        return None\n",
      "\n",
      "    multiplefn = match.group(1)\n",
      "    root = ET.fromstring(multiplefn)\n",
      "    functions = root.findall(\"functioncall\")\n",
      "\n",
      "    return [json.loads(fn.text) for fn in functions]\n",
      "\n",
      "\n",
      "def invoke_function(function, feature_view, weather_fg, model) -> pd.DataFrame:\n",
      "    \"\"\"Invoke a function with given arguments.\"\"\"\n",
      "    # Extract function name and arguments from input_data\n",
      "    function_name = function['name']\n",
      "    arguments = function['arguments']\n",
      "\n",
      "    # Using Python's getattr function to dynamically call the function by its name and passing the arguments\n",
      "    function_output = getattr(sys.modules[__name__], function_name)(\n",
      "        **arguments,\n",
      "        feature_view=feature_view,\n",
      "        weather_fg=weather_fg,\n",
      "        model=model,\n",
      "    )\n",
      "\n",
      "    if type(function_output) == str:\n",
      "        return function_output\n",
      "\n",
      "    # Round the 'pm25' value to 2 decimal places\n",
      "    function_output['pm25'] = function_output['pm25'].apply(round, ndigits=2)\n",
      "    return function_output\n",
      "\n",
      "\n",
      "def get_context_data(user_query: str, feature_view, weather_fg, model_air_quality, model_llm=None, tokenizer=None, client=None) -> str:\n",
      "    \"\"\"\n",
      "    Retrieve context data based on user query.\n",
      "\n",
      "    Args:\n",
      "        user_query (str): The user query.\n",
      "        feature_view: Feature View for data retrieval.\n",
      "        model_air_quality: The air quality model.\n",
      "        tokenizer: The tokenizer.\n",
      "\n",
      "    Returns:\n",
      "        str: The context data.\n",
      "    \"\"\"\n",
      "    if client:\n",
      "        # Generate a response using LLM\n",
      "        completion = function_calling_with_openai(user_query, client)\n",
      "\n",
      "    else:\n",
      "        # Generate a response using LLM\n",
      "        completion = generate_hermes(\n",
      "            user_query,\n",
      "            model_llm,\n",
      "            tokenizer,\n",
      "        )\n",
      "\n",
      "    # Extract function calls from the completion\n",
      "    functions = extract_function_calls(completion)\n",
      "\n",
      "    # If function calls were found\n",
      "    if functions:\n",
      "        # Invoke the function with provided arguments\n",
      "        data = invoke_function(functions[0], feature_view, weather_fg, model_air_quality)\n",
      "\n",
      "        # Return formatted data as string\n",
      "        if isinstance(data, pd.DataFrame):\n",
      "            return f'Air Quality Measurements:\\n' + '\\n'.join(\n",
      "                [f'Date: {row[\"date\"]}; Air Quality: {row[\"pm25\"]}' for _, row in data.iterrows()]\n",
      "            )\n",
      "        # Return message if data is not updated\n",
      "        return data\n",
      "\n",
      "    # If no function calls were found, return an empty string\n",
      "    return ''\n",
      "import transformers\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig, AutoModel\n",
      "from langchain.llms import HuggingFacePipeline\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains.llm import LLMChain\n",
      "from langchain.memory import ConversationBufferWindowMemory\n",
      "import torch\n",
      "import datetime\n",
      "from typing import Any, Dict, Union\n",
      "from functions.context_engineering import get_context_data\n",
      "import os\n",
      "from safetensors.torch import load_model, save_model\n",
      "\n",
      "def load_model(model_id: str = \"teknium/OpenHermes-2.5-Mistral-7B\") -> tuple:\n",
      "    \"\"\"\n",
      "    Load the LLM and its corresponding tokenizer.\n",
      "\n",
      "    Args:\n",
      "        model_id (str, optional): Identifier for the pre-trained model. Defaults to \"teknium/OpenHermes-2.5-Mistral-7B\".\n",
      "\n",
      "    Returns:\n",
      "        tuple: A tuple containing the loaded model and tokenizer.\n",
      "    \"\"\"\n",
      "\n",
      "    # Load the tokenizer for Mistral-7B-Instruct model\n",
      "    tokenizer_path = \"./mistral/tokenizer\"\n",
      "    if os.path.isdir(tokenizer_path) == False:\n",
      "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "        tokenizer.save_pretrained(tokenizer_path)\n",
      "    else:\n",
      "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
      "\n",
      "    # Set the pad token to the unknown token to handle padding\n",
      "    tokenizer.pad_token = tokenizer.unk_token\n",
      "\n",
      "    # Set the padding side to \"right\" to prevent warnings during tokenization\n",
      "    tokenizer.padding_side = \"right\"\n",
      "\n",
      "    # BitsAndBytesConfig int-4 config\n",
      "    bnb_config = BitsAndBytesConfig(\n",
      "        load_in_4bit=True,\n",
      "        bnb_4bit_use_double_quant=True,\n",
      "        bnb_4bit_quant_type=\"nf4\",\n",
      "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
      "    )\n",
      "\n",
      "    model_path = \"/tmp/mistral/model\"\n",
      "    if os.path.exists(model_path):\n",
      "        print(\"Loading model from disk\")\n",
      "        model_llm = AutoModelForCausalLM.from_pretrained(model_path)\n",
      "    else:\n",
      "        # Load the Mistral-7B-Instruct model with quantization configuration\n",
      "        model_llm = AutoModelForCausalLM.from_pretrained(\n",
      "            model_id,\n",
      "            device_map=\"auto\",\n",
      "            quantization_config=bnb_config,\n",
      "        )\n",
      "        model_llm.save_pretrained(model_path)\n",
      "\n",
      "\n",
      "    # Configure the pad token ID in the model to match the tokenizer's pad token ID\n",
      "    model_llm.config.pad_token_id = tokenizer.pad_token_id\n",
      "\n",
      "    return model_llm, tokenizer\n",
      "\n",
      "\n",
      "def get_prompt_template():\n",
      "    \"\"\"\n",
      "    Retrieve a template for generating prompts in a conversational AI system.\n",
      "\n",
      "    Returns:\n",
      "        str: A string representing the template for generating prompts.\n",
      "            This template includes placeholders for system information,\n",
      "            instructions, previous conversation, context, date and user query.\n",
      "    \"\"\"\n",
      "    prompt_template = \"\"\"<|im_start|>system\n",
      "You are one of the best air quality experts in the world.\n",
      "\n",
      "###INSTRUCTIONS:\n",
      "- If you don't know the answer, you will respond politely that you cannot help.\n",
      "- Use the context table with air quality indicators for city provided by user to generate your answer.\n",
      "- You answer should be at least one sentence.\n",
      "- Do not show any calculations to the user.\n",
      "- Make sure that you use correct air quality indicators for the corresponding date.\n",
      "- Add a rich analysis of the air quality level, such as whether it is safe, whether to go for a walk, etc.\n",
      "- Do not mention in your answer that you are using context table.\n",
      "<|im_end|>\n",
      "\n",
      "### CONTEXT:\n",
      "{context}\n",
      "\n",
      "IMPORTANT: Today is {date_today}.\n",
      "\n",
      "<|im_start|>user\n",
      "{question}<|im_end|>\n",
      "<|im_start|>assistant\"\"\"\n",
      "    return prompt_template\n",
      "\n",
      "\n",
      "def get_llm_chain(model_llm, tokenizer):\n",
      "    \"\"\"\n",
      "    Create and configure a language model chain.\n",
      "\n",
      "    Args:\n",
      "        model_llm: The pre-trained language model for text generation.\n",
      "        tokenizer: The tokenizer corresponding to the language model.\n",
      "\n",
      "    Returns:\n",
      "        LLMChain: The configured language model chain.\n",
      "    \"\"\"\n",
      "    # Create a text generation pipeline using the loaded model and tokenizer\n",
      "    text_generation_pipeline = transformers.pipeline(\n",
      "        model=model_llm,                      # The pre-trained language model for text generation\n",
      "        tokenizer=tokenizer,                  # The tokenizer corresponding to the language model\n",
      "        task=\"text-generation\",               # Specify the task as text generation\n",
      "        use_cache=True,\n",
      "        do_sample=True,\n",
      "        temperature=0.4,\n",
      "        top_p=1.0,\n",
      "        top_k=0,\n",
      "        max_new_tokens=512,\n",
      "        eos_token_id=tokenizer.eos_token_id,\n",
      "        pad_token_id=tokenizer.eos_token_id,\n",
      "    )\n",
      "\n",
      "    # Create a Hugging Face pipeline for Mistral LLM using the text generation pipeline\n",
      "    mistral_llm = HuggingFacePipeline(\n",
      "        pipeline=text_generation_pipeline,\n",
      "    )\n",
      "\n",
      "    # Create prompt from prompt template\n",
      "    prompt = PromptTemplate(\n",
      "        input_variables=[\"context\", \"question\", \"date_today\"],\n",
      "        template=get_prompt_template(),\n",
      "    )\n",
      "\n",
      "    # Create LLM chain\n",
      "    llm_chain = LLMChain(\n",
      "        llm=mistral_llm,\n",
      "        prompt=prompt,\n",
      "        verbose=False,\n",
      "    )\n",
      "\n",
      "    return llm_chain\n",
      "\n",
      "\n",
      "def generate_response(\n",
      "    user_query: str,\n",
      "    feature_view,\n",
      "    weather_fg,\n",
      "    model_air_quality,\n",
      "    model_llm,\n",
      "    tokenizer,\n",
      "    llm_chain=None,\n",
      "    verbose: bool = False,\n",
      ") -> str:\n",
      "    \"\"\"\n",
      "    Generate response to user query using LLM chain and context data.\n",
      "\n",
      "    Args:\n",
      "        user_query (str): The user's query.\n",
      "        feature_view: Feature view for data retrieval.\n",
      "        model_llm: Language model for text generation.\n",
      "        tokenizer: Tokenizer for processing text.\n",
      "        model_air_quality: Model for predicting air quality.\n",
      "        llm_chain: LLM Chain.\n",
      "        verbose (bool): Whether to print verbose information. Defaults to False.\n",
      "\n",
      "    Returns:\n",
      "        str: Generated response to the user query.\n",
      "    \"\"\"\n",
      "    # Get context data based on user query\n",
      "    context = get_context_data(\n",
      "        user_query,\n",
      "        feature_view,\n",
      "        weather_fg,\n",
      "        model_air_quality,\n",
      "        model_llm=model_llm,\n",
      "        tokenizer=tokenizer,\n",
      "    )\n",
      "\n",
      "    # Get today's date in a readable format\n",
      "    date_today = f'{datetime.date.today().strftime(\"%A\")}, {datetime.date.today()}'\n",
      "\n",
      "    # Print today's date and context information if verbose mode is enabled\n",
      "    if verbose:\n",
      "        print(f\"üóìÔ∏è Today's date: {date_today}\")\n",
      "        print(f'üìñ {context}')\n",
      "\n",
      "    # Invoke the language model chain with relevant context\n",
      "    model_output = llm_chain.invoke({\n",
      "        \"context\": context,\n",
      "        \"date_today\": date_today,\n",
      "        \"question\": user_query,\n",
      "    })\n",
      "\n",
      "    # Return the generated text from the model output\n",
      "    return model_output['text'].split('<|im_start|>assistant')[-1]\n",
      "\n",
      "\n",
      "def generate_response_openai(\n",
      "    user_query: str,\n",
      "    feature_view,\n",
      "    weather_fg,\n",
      "    model_air_quality,\n",
      "    client,\n",
      "    verbose=True,\n",
      "):\n",
      "\n",
      "    context = get_context_data(\n",
      "        user_query,\n",
      "        feature_view,\n",
      "        weather_fg,\n",
      "        model_air_quality,\n",
      "        client=client,\n",
      "    )\n",
      "\n",
      "    # Get today's date in a readable format\n",
      "    date_today = f'{datetime.date.today().strftime(\"%A\")}, {datetime.date.today()}'\n",
      "\n",
      "    # Print today's date and context information if verbose mode is enabled\n",
      "    if verbose:\n",
      "        print(f\"üóìÔ∏è Today's date: {date_today}\")\n",
      "        print(f'üìñ {context}')\n",
      "\n",
      "    instructions = get_prompt_template().split('<|im_start|>user')[0]\n",
      "\n",
      "    instructions_filled = instructions.format(\n",
      "        context=context,\n",
      "        date_today=date_today\n",
      "    )\n",
      "\n",
      "    completion = client.chat.completions.create(\n",
      "        model=\"gpt-4-0125-preview\",\n",
      "        messages=[\n",
      "            {\"role\": \"system\", \"content\": instructions_filled},\n",
      "            {\"role\": \"user\", \"content\": user_query},\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    # Extract and return the assistant's reply from the response\n",
      "    if completion and completion.choices:\n",
      "        last_choice = completion.choices[0]\n",
      "        if last_choice.message:\n",
      "            return last_choice.message.content.strip()\n",
      "    return \"\"\n",
      "import os\n",
      "import datetime\n",
      "import time\n",
      "import requests\n",
      "import pandas as pd\n",
      "import json\n",
      "from geopy.geocoders import Nominatim\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.patches import Patch\n",
      "from matplotlib.ticker import MultipleLocator\n",
      "import openmeteo_requests\n",
      "import requests_cache\n",
      "from retry_requests import retry\n",
      "import hopsworks\n",
      "import hsfs\n",
      "from pathlib import Path\n",
      "\n",
      "def get_historical_weather(city, start_date,  end_date, latitude, longitude):\n",
      "    # latitude, longitude = get_city_coordinates(city)\n",
      "\n",
      "    # Setup the Open-Meteo API client with cache and retry on error\n",
      "    cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
      "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
      "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
      "\n",
      "    # Make sure all required weather variables are listed here\n",
      "    # The order of variables in hourly or daily is important to assign them correctly below\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    params = {\n",
      "        \"latitude\": latitude,\n",
      "        \"longitude\": longitude,\n",
      "        \"start_date\": start_date,\n",
      "        \"end_date\": end_date,\n",
      "        \"daily\": [\"temperature_2m_mean\", \"precipitation_sum\", \"wind_speed_10m_max\", \"wind_direction_10m_dominant\"]\n",
      "    }\n",
      "    responses = openmeteo.weather_api(url, params=params)\n",
      "\n",
      "    # Process first location. Add a for-loop for multiple locations or weather models\n",
      "    response = responses[0]\n",
      "    print(f\"Coordinates {response.Latitude()}¬∞N {response.Longitude()}¬∞E\")\n",
      "    print(f\"Elevation {response.Elevation()} m asl\")\n",
      "    print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
      "    print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
      "\n",
      "    # Process daily data. The order of variables needs to be the same as requested.\n",
      "    daily = response.Daily()\n",
      "    daily_temperature_2m_mean = daily.Variables(0).ValuesAsNumpy()\n",
      "    daily_precipitation_sum = daily.Variables(1).ValuesAsNumpy()\n",
      "    daily_wind_speed_10m_max = daily.Variables(2).ValuesAsNumpy()\n",
      "    daily_wind_direction_10m_dominant = daily.Variables(3).ValuesAsNumpy()\n",
      "\n",
      "    daily_data = {\"date\": pd.date_range(\n",
      "        start = pd.to_datetime(daily.Time(), unit = \"s\"),\n",
      "        end = pd.to_datetime(daily.TimeEnd(), unit = \"s\"),\n",
      "        freq = pd.Timedelta(seconds = daily.Interval()),\n",
      "        inclusive = \"left\"\n",
      "    )}\n",
      "    daily_data[\"temperature_2m_mean\"] = daily_temperature_2m_mean\n",
      "    daily_data[\"precipitation_sum\"] = daily_precipitation_sum\n",
      "    daily_data[\"wind_speed_10m_max\"] = daily_wind_speed_10m_max\n",
      "    daily_data[\"wind_direction_10m_dominant\"] = daily_wind_direction_10m_dominant\n",
      "\n",
      "    daily_dataframe = pd.DataFrame(data = daily_data)\n",
      "    daily_dataframe = daily_dataframe.dropna()\n",
      "    daily_dataframe['city'] = city\n",
      "    return daily_dataframe\n",
      "\n",
      "def get_hourly_weather_forecast(city, latitude, longitude):\n",
      "\n",
      "    # latitude, longitude = get_city_coordinates(city)\n",
      "\n",
      "    # Setup the Open-Meteo API client with cache and retry on error\n",
      "    cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
      "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
      "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
      "\n",
      "    # Make sure all required weather variables are listed here\n",
      "    # The order of variables in hourly or daily is important to assign them correctly below\n",
      "    url = \"https://api.open-meteo.com/v1/ecmwf\"\n",
      "    params = {\n",
      "        \"latitude\": latitude,\n",
      "        \"longitude\": longitude,\n",
      "        \"hourly\": [\"temperature_2m\", \"precipitation\", \"wind_speed_10m\", \"wind_direction_10m\"]\n",
      "    }\n",
      "    responses = openmeteo.weather_api(url, params=params)\n",
      "\n",
      "    # Process first location. Add a for-loop for multiple locations or weather models\n",
      "    response = responses[0]\n",
      "    print(f\"Coordinates {response.Latitude()}¬∞N {response.Longitude()}¬∞E\")\n",
      "    print(f\"Elevation {response.Elevation()} m asl\")\n",
      "    print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
      "    print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
      "\n",
      "    # Process hourly data. The order of variables needs to be the same as requested.\n",
      "\n",
      "    hourly = response.Hourly()\n",
      "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
      "    hourly_precipitation = hourly.Variables(1).ValuesAsNumpy()\n",
      "    hourly_wind_speed_10m = hourly.Variables(2).ValuesAsNumpy()\n",
      "    hourly_wind_direction_10m = hourly.Variables(3).ValuesAsNumpy()\n",
      "\n",
      "    hourly_data = {\"date\": pd.date_range(\n",
      "        start = pd.to_datetime(hourly.Time(), unit = \"s\"),\n",
      "        end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\"),\n",
      "        freq = pd.Timedelta(seconds = hourly.Interval()),\n",
      "        inclusive = \"left\"\n",
      "    )}\n",
      "    hourly_data[\"temperature_2m_mean\"] = hourly_temperature_2m\n",
      "    hourly_data[\"precipitation_sum\"] = hourly_precipitation\n",
      "    hourly_data[\"wind_speed_10m_max\"] = hourly_wind_speed_10m\n",
      "    hourly_data[\"wind_direction_10m_dominant\"] = hourly_wind_direction_10m\n",
      "\n",
      "    hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
      "    hourly_dataframe = hourly_dataframe.dropna()\n",
      "    return hourly_dataframe\n",
      "\n",
      "\n",
      "\n",
      "def get_city_coordinates(city_name: str):\n",
      "    \"\"\"\n",
      "    Takes city name and returns its latitude and longitude (rounded to 2 digits after dot).\n",
      "    \"\"\"\n",
      "    # Initialize Nominatim API (for getting lat and long of the city)\n",
      "    geolocator = Nominatim(user_agent=\"MyApp\")\n",
      "    city = geolocator.geocode(city_name)\n",
      "\n",
      "    latitude = round(city.latitude, 2)\n",
      "    longitude = round(city.longitude, 2)\n",
      "\n",
      "    return latitude, longitude\n",
      "\n",
      "def trigger_request(url:str):\n",
      "    response = requests.get(url)\n",
      "    if response.status_code == 200:\n",
      "        # Extract the JSON content from the response\n",
      "        data = response.json()\n",
      "    else:\n",
      "        print(\"Failed to retrieve data. Status Code:\", response.status_code)\n",
      "        raise requests.exceptions.RequestException(response.status_code)\n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "def get_pm25(aqicn_url: str, country: str, city: str, street: str, day: datetime.date, AQI_API_KEY: str):\n",
      "    \"\"\"\n",
      "    Returns DataFrame with air quality (pm25) as dataframe\n",
      "    \"\"\"\n",
      "    # The API endpoint URL\n",
      "    url = f\"{aqicn_url}/?token={AQI_API_KEY}\"\n",
      "\n",
      "    # Make a GET request to fetch the data from the API\n",
      "    data = trigger_request(url)\n",
      "\n",
      "    # if we get 'Unknown station' response then retry with city in url\n",
      "    if data['data'] == \"Unknown station\":\n",
      "        url1 = f\"https://api.waqi.info/feed/{country}/{street}/?token={AQI_API_KEY}\"\n",
      "        data = trigger_request(url1)\n",
      "\n",
      "    if data['data'] == \"Unknown station\":\n",
      "        url2 = f\"https://api.waqi.info/feed/{country}/{city}/{street}/?token={AQI_API_KEY}\"\n",
      "        data = trigger_request(url2)\n",
      "\n",
      "\n",
      "    # Check if the API response contains the data\n",
      "    if data['status'] == 'ok':\n",
      "        # Extract the air quality data\n",
      "        aqi_data = data['data']\n",
      "        aq_today_df = pd.DataFrame()\n",
      "        aq_today_df['pm25'] = [aqi_data['iaqi'].get('pm25', {}).get('v', None)]\n",
      "        aq_today_df['pm25'] = aq_today_df['pm25'].astype('float32')\n",
      "\n",
      "        aq_today_df['country'] = country\n",
      "        aq_today_df['city'] = city\n",
      "        aq_today_df['street'] = street\n",
      "        aq_today_df['date'] = day\n",
      "        aq_today_df['date'] = pd.to_datetime(aq_today_df['date'])\n",
      "        aq_today_df['url'] = aqicn_url\n",
      "    else:\n",
      "        print(\"Error: There may be an incorrect  URL for your Sensor or it is not contactable right now. The API response does not contain data.  Error message:\", data['data'])\n",
      "        raise requests.exceptions.RequestException(data['data'])\n",
      "\n",
      "    return aq_today_df\n",
      "\n",
      "\n",
      "def plot_air_quality_forecast(city: str, street: str, df: pd.DataFrame, file_path: str, hindcast=False):\n",
      "    fig, ax = plt.subplots(figsize=(10, 6))\n",
      "\n",
      "    day = pd.to_datetime(df['date']).dt.date\n",
      "    # Plot each column separately in matplotlib\n",
      "    ax.plot(day, df['predicted_pm25'], label='Predicted PM2.5', color='red', linewidth=2, marker='o', markersize=5, markerfacecolor='blue')\n",
      "\n",
      "    # Set the y-axis to a logarithmic scale\n",
      "    ax.set_yscale('log')\n",
      "    ax.set_yticks([0, 10, 25, 50, 100, 250, 500])\n",
      "    ax.get_yaxis().set_major_formatter(plt.ScalarFormatter())\n",
      "    ax.set_ylim(bottom=1)\n",
      "\n",
      "    # Set the labels and title\n",
      "    ax.set_xlabel('Date')\n",
      "    ax.set_title(f\"PM2.5 Predicted (Logarithmic Scale) for {city}, {street}\")\n",
      "    ax.set_ylabel('PM2.5')\n",
      "\n",
      "    colors = ['green', 'yellow', 'orange', 'red', 'purple', 'darkred']\n",
      "    labels = ['Good', 'Moderate', 'Unhealthy for Some', 'Unhealthy', 'Very Unhealthy', 'Hazardous']\n",
      "    ranges = [(0, 49), (50, 99), (100, 149), (150, 199), (200, 299), (300, 500)]\n",
      "    for color, (start, end) in zip(colors, ranges):\n",
      "        ax.axhspan(start, end, color=color, alpha=0.3)\n",
      "\n",
      "    # Add a legend for the different Air Quality Categories\n",
      "    patches = [Patch(color=colors[i], label=f\"{labels[i]}: {ranges[i][0]}-{ranges[i][1]}\") for i in range(len(colors))]\n",
      "    legend1 = ax.legend(handles=patches, loc='upper right', title=\"Air Quality Categories\", fontsize='x-small')\n",
      "\n",
      "    # Aim for ~10 annotated values on x-axis, will work for both forecasts ans hindcasts\n",
      "    if len(df.index) > 11:\n",
      "        every_x_tick = len(df.index) / 10\n",
      "        ax.xaxis.set_major_locator(MultipleLocator(every_x_tick))\n",
      "\n",
      "    plt.xticks(rotation=45)\n",
      "\n",
      "    if hindcast == True:\n",
      "        ax.plot(day, df['pm25'], label='Actual PM2.5', color='black', linewidth=2, marker='^', markersize=5, markerfacecolor='grey')\n",
      "        legend2 = ax.legend(loc='upper left', fontsize='x-small')\n",
      "        ax.add_artist(legend1)\n",
      "\n",
      "    # Ensure everything is laid out neatly\n",
      "    plt.tight_layout()\n",
      "\n",
      "    # # Save the figure, overwriting any existing file with the same name\n",
      "    plt.savefig(file_path)\n",
      "    return plt\n",
      "\n",
      "\n",
      "def delete_feature_groups(fs, name):\n",
      "    try:\n",
      "        for fg in fs.get_feature_groups(name):\n",
      "            fg.delete()\n",
      "            print(f\"Deleted {fg.name}/{fg.version}\")\n",
      "    except hsfs.client.exceptions.RestAPIError:\n",
      "        print(f\"No {name} feature group found\")\n",
      "\n",
      "def delete_feature_views(fs, name):\n",
      "    try:\n",
      "        for fv in fs.get_feature_views(name):\n",
      "            fv.delete()\n",
      "            print(f\"Deleted {fv.name}/{fv.version}\")\n",
      "    except hsfs.client.exceptions.RestAPIError:\n",
      "        print(f\"No {name} feature view found\")\n",
      "\n",
      "def delete_models(mr, name):\n",
      "    models = mr.get_models(name)\n",
      "    if not models:\n",
      "        print(f\"No {name} model found\")\n",
      "    for model in models:\n",
      "        model.delete()\n",
      "        print(f\"Deleted model {model.name}/{model.version}\")\n",
      "\n",
      "def delete_secrets(proj, name):\n",
      "    secrets = secrets_api(proj.name)\n",
      "    try:\n",
      "        secret = secrets.get_secret(name)\n",
      "        secret.delete()\n",
      "        print(f\"Deleted secret {name}\")\n",
      "    except hopsworks.client.exceptions.RestAPIError:\n",
      "        print(f\"No {name} secret found\")\n",
      "\n",
      "# WARNING - this will wipe out all your feature data and models\n",
      "def purge_project(proj):\n",
      "    fs = proj.get_feature_store()\n",
      "    mr = proj.get_model_registry()\n",
      "\n",
      "    # Delete Feature Views before deleting the feature groups\n",
      "    delete_feature_views(fs, \"air_quality_fv\")\n",
      "\n",
      "    # Delete ALL Feature Groups\n",
      "    delete_feature_groups(fs, \"air_quality\")\n",
      "    delete_feature_groups(fs, \"weather\")\n",
      "    delete_feature_groups(fs, \"aq_predictions\")\n",
      "\n",
      "    # Delete all Models\n",
      "    delete_models(mr, \"air_quality_xgboost_model\")\n",
      "    delete_secrets(proj, \"SENSOR_LOCATION_JSON\")\n",
      "\n",
      "def check_file_path(file_path):\n",
      "    my_file = Path(file_path)\n",
      "    if my_file.is_file() == False:\n",
      "        print(f\"Error. File not found at the path: {file_path} \")\n",
      "    else:\n",
      "        print(f\"File successfully found at the path: {file_path}\")\n",
      "\n",
      "def backfill_predictions_for_monitoring(weather_fg, air_quality_df, monitor_fg, model):\n",
      "    features_df = weather_fg.read()\n",
      "    features_df = features_df.sort_values(by=['date'], ascending=True)\n",
      "    features_df = features_df.tail(10)\n",
      "    features_df['predicted_pm25'] = model.predict(features_df[['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']])\n",
      "    df = pd.merge(features_df, air_quality_df[['date','pm25','street','country']], on=\"date\")\n",
      "    df['days_before_forecast_day'] = 1\n",
      "    hindcast_df = df\n",
      "    df = df.drop('pm25', axis=1)\n",
      "    monitor_fg.insert(df, write_options={\"wait_for_job\": True})\n",
      "    return hindcast_df\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p functions\n",
    "!cd functions && curl https://raw.githubusercontent.com/featurestorebook/mlfs-book/main/notebooks/ch03/functions/air_quality_data_retrieval.py \n",
    "!cd functions && curl https://raw.githubusercontent.com/featurestorebook/mlfs-book/main/notebooks/ch03/functions/context_engineering.py\n",
    "!cd functions && curl https://raw.githubusercontent.com/featurestorebook/mlfs-book/main/notebooks/ch03/functions/llm_chain.py\n",
    "!cd functions && curl https://raw.githubusercontent.com/featurestorebook/mlfs-book/main/notebooks/ch03/functions/util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3f016",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721ae546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ymm/opt/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import hopsworks\n",
    "from openai import OpenAI\n",
    "from functions.llm_chain import (\n",
    "    load_model, \n",
    "    get_llm_chain, \n",
    "    generate_response, \n",
    "    generate_response_openai,\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b062cc0",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6340e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:41:35,916 INFO: Initializing external client\n",
      "2024-12-15 03:41:35,917 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-15 03:41:38,557 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1164445\n"
     ]
    }
   ],
   "source": [
    "# If you haven't set the env variable 'HOPSWORKS_API_KEY', then uncomment the next line and enter your API key\n",
    "# os.environ[\"HOPSWORKS_API_KEY\"] = \"\"\n",
    "# with open('../../data/hopsworks-api-key.txt', 'r') as file:\n",
    "#     os.environ[\"HOPSWORKS_API_KEY\"] = file.read().rstrip()\n",
    "with open('../../data/hopsworks-api-key.txt', 'r') as file:\n",
    "    os.environ[\"HOPSWORKS_API_KEY\"] = file.read().rstrip()\n",
    "\n",
    "\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f2f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get_or_create the 'air_quality_fv' feature view\n",
    "feature_view = fs.get_feature_view(\n",
    "    name='air_quality_fv',\n",
    "    version=4\n",
    ")\n",
    "\n",
    "# Initialize batch scoring\n",
    "feature_view.init_batch_scoring(1)\n",
    "\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59585afd-64be-4ad9-bb61-f20eb21af17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models with name 'air_quality_xgboost_model_with_lagged_data': [Model(name: 'air_quality_xgboost_model_with_lagged_data', version: 3), Model(name: 'air_quality_xgboost_model_with_lagged_data', version: 1), Model(name: 'air_quality_xgboost_model_with_lagged_data', version: 2)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'list_versions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m models:\n\u001b[1;32m     11\u001b[0m     model \u001b[38;5;241m=\u001b[39m mr\u001b[38;5;241m.\u001b[39mget_model(name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable versions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_versions\u001b[49m())\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models found with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'list_versions'"
     ]
    }
   ],
   "source": [
    "# Ê£ÄÊü•ÂèØÁî®ÁöÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8002765b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™ù Retrieve AirQuality Model from Model Registry</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02695f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (2 dirs, 6 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "# Retrieve the model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "retrieved_model = mr.get_model(\n",
    "    name=\"air_quality_xgboost_model_with_lagged_data\",\n",
    "    version=3,\n",
    ")\n",
    "\n",
    "# Download the saved model artifacts to a local directory\n",
    "saved_model_dir = retrieved_model.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa0e4256-fcbe-4372-89cc-0d592177bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:44:14,875 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2024-12-15 03:44:14,899 INFO: Initializing external client\n",
      "2024-12-15 03:44:14,900 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-15 03:44:16,254 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1164445\n",
      "Base path: /Projects/la/Models/air_quality_xgboost_model_with_lagged_data\n",
      "\n",
      "Trying different paths:\n",
      "\n",
      "Attempting path: /Projects/la/Models/air_quality_xgboost_model_with_lagged_data/3\n",
      "Success! Model downloaded to: ./model_files__Projects_la_Models_air_quality_xgboost_model_with_lagged_data_3\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "import os\n",
    "\n",
    "# ÈáçÊñ∞ËøûÊé•\n",
    "project = hopsworks.login()\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "try:\n",
    "    # Ëé∑ÂèñÊ®°Âûã\n",
    "    model = mr.get_model(\n",
    "        name=\"air_quality_xgboost_model_with_lagged_data\",\n",
    "        version=3\n",
    "    )\n",
    "    \n",
    "    # Â∞ùËØï‰ΩøÁî®‰∏çÂêåÁöÑË∑ØÂæÑÊ†ºÂºè\n",
    "    model_base_path = model.model_path\n",
    "    print(f\"Base path: {model_base_path}\")\n",
    "    \n",
    "    # Â∞ùËØï‰∏çÂêåÁöÑË∑ØÂæÑÁªÑÂêà\n",
    "    possible_paths = [\n",
    "        f\"{model_base_path}/3\",\n",
    "        f\"{model_base_path}/v3\",\n",
    "        f\"{model_base_path}_v3\",\n",
    "        f\"{model_base_path}/version_3\",\n",
    "        model_base_path  # Â∞ùËØïÁõ¥Êé•‰ΩøÁî®Âü∫Êú¨Ë∑ØÂæÑ\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTrying different paths:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"\\nAttempting path: {path}\")\n",
    "        try:\n",
    "            local_dir = f'./model_files_{path.replace(\"/\", \"_\")}'\n",
    "            os.makedirs(local_dir, exist_ok=True)\n",
    "            saved_model_dir = model.download(local_path=local_dir)\n",
    "            print(f\"Success! Model downloaded to: {saved_model_dir}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with: {str(e)}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Main error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8930caa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=&#x27;2.327862E1&#x27;, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=[&#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;,\n",
       "                            &#x27;float&#x27;, &#x27;float&#x27;],\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=&#x27;2.327862E1&#x27;, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=[&#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;,\n",
       "                            &#x27;float&#x27;, &#x27;float&#x27;],\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score='2.327862E1', booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=['float', 'float', 'float', 'float', 'float',\n",
       "                            'float', 'float'],\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the XGBoost regressor model and label encoder from the saved model directory\n",
    "# model_air_quality = joblib.load(saved_model_dir + \"/xgboost_regressor.pkl\")\n",
    "model_air_quality = XGBRegressor()\n",
    "\n",
    "model_air_quality.load_model(saved_model_dir + \"/model.json\")\n",
    "\n",
    "# Displaying the retrieved XGBoost regressor model\n",
    "model_air_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30142d",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚¨áÔ∏è LLM Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a911a86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the LLM and its corresponding tokenizer.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model_llm, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimiraoui/OpenHermes-2.5-Mistral-7B-sharded\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe code execution took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mlfs-book/notebooks/ch03/functions/llm_chain.py:53\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_id)\u001b[0m\n\u001b[1;32m     50\u001b[0m     model_llm \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Load the Mistral-7B-Instruct model with quantization configuration\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     model_llm \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     model_llm\u001b[38;5;241m.\u001b[39msave_pretrained(model_path)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Configure the pad token ID in the model to match the tokenizer's pad token ID\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/modeling_utils.py:3024\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3027\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3028\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the LLM and its corresponding tokenizer.\n",
    "model_llm, tokenizer = load_model(model_id=\"imiraoui/OpenHermes-2.5-Mistral-7B-sharded\")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e329285",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚õìÔ∏è LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Create and configure a language model chain.\n",
    "llm_chain = get_llm_chain(\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ded5c",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>üß¨ Domain-specific Evaluation Harness\n",
    "\n",
    "**Systematic evaluations** that can run automatically in CI/CD pipelines are key to evaluating models/RAG. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58181b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION7 = \"Hi!\"\n",
    "\n",
    "response7 = generate_response(\n",
    "    QUESTION7,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec32e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Who are you?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION1 = \"What was the average air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response1 = generate_response(\n",
    "    QUESTION1, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d01dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION11 = \"When and what was the air quality like last week?\"\n",
    "\n",
    "response11 = generate_response(\n",
    "    QUESTION11, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION12 = \"When and what was the minimum air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response12 = generate_response(\n",
    "    QUESTION12, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2a = \"What was the air quality like last week?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2a,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2 = \"What was the air quality like yesterday?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed349483",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION3 = \"What will the air quality be like next Tuesday?\"\n",
    "\n",
    "response3 = generate_response(\n",
    "    QUESTION3, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6825c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION4 = \"What will the air quality be like the day after tomorrow?\"\n",
    "\n",
    "response4 = generate_response(\n",
    "    QUESTION4, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION5 = \"What will the air quality be like this Sunday?\"\n",
    "\n",
    "response5 = generate_response(\n",
    "    QUESTION5, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee271416",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION7 = \"What will the air quality be like for the rest of the week?\"\n",
    "\n",
    "response7 = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeeb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Will the air quality be safe or not for the next week?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Is tomorrow's air quality level dangerous?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb26726",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Can you please explain different PM2_5 air quality levels?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a463f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai --quiet\n",
    "# !pip install gradio==3.40.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4aebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from functions.llm_chain import load_model, get_llm_chain, generate_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    if y.ndim > 1 and y.shape[1] > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "    y /= np.max(np.abs(y))\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
    "\n",
    "def generate_query_response(user_query, method, openai_api_key=None):\n",
    "    if method == 'Hermes LLM':        \n",
    "        response = generate_response(\n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            model_llm,\n",
    "            tokenizer,\n",
    "            llm_chain,\n",
    "            verbose=False,\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    elif method == 'OpenAI API' and openai_api_key:\n",
    "        client = OpenAI(\n",
    "            api_key=openai_api_key\n",
    "        )\n",
    "        \n",
    "        response = generate_response_openai(   \n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            client=client,\n",
    "            verbose=True,\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    else:\n",
    "        return \"Invalid method or missing API key.\"\n",
    "\n",
    "def handle_input(text_input=None, audio_input=None, method='Hermes LLM', openai_api_key=\"\"):\n",
    "    if audio_input is not None:\n",
    "        user_query = transcribe(audio_input)\n",
    "    else:\n",
    "        user_query = text_input\n",
    "    \n",
    "    # Check if OpenAI API key is required but not provided\n",
    "    if method == 'OpenAI API' and not openai_api_key.strip():\n",
    "        return \"OpenAI API key is required for this method.\"\n",
    "\n",
    "    if user_query:\n",
    "        return generate_query_response(user_query, method, openai_api_key)\n",
    "    else:\n",
    "        return \"Please provide input either via text or voice.\"\n",
    "    \n",
    "\n",
    "# Setting up the Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=handle_input,\n",
    "    inputs=[\n",
    "        gr.Textbox(placeholder=\"Type here or use voice input...\"), \n",
    "        gr.Audio(), \n",
    "        gr.Radio([\"Hermes LLM\", \"OpenAI API\"], label=\"Choose the response generation method\"),\n",
    "        gr.Textbox(label=\"Enter your OpenAI API key (only if you selected OpenAI API):\", type=\"password\")  # Removed `optional=True`\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"üå§Ô∏è AirQuality AI Assistant üí¨\",\n",
    "    description=\"Ask your questions about air quality or use your voice to interact. Select the response generation method and provide an OpenAI API key if necessary.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa7c6a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
